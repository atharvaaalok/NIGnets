{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Monotonic Networks\n",
    "subtitle: Neural Networks as Activation Functions\n",
    "date: 2025-02-26\n",
    "bibliography:\n",
    "  - monotonic_networks_references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import geosimilarity as gs\n",
    "from NIGnets import NIGnet\n",
    "\n",
    "from assets.utils import automate_training, plot_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Injective Functions with Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed earlier that with the [`PReLU`](#parameter_insight) activation Injective Networks\n",
    "perform much better. Therefore, the insight we gain that will help us power up Injective Networks is\n",
    ": **add more parameters**.\n",
    "\n",
    "One way to add parameters to the Network is to use parameterized activation functions e.g.\n",
    "$\\sigma(\\beta x)$ i.e. the sigmoid with a parameter $\\beta$ or try and create other activations.\n",
    "During the creation of these parameterized activations one needs to keep in mind that injectivity\n",
    "should not be violated for any parameter value to ensure that the network always represents only\n",
    "simple closed curves, which is our final goal.\n",
    "\n",
    "It is quite difficult to combine activation functions together with appropriate parameters that add\n",
    "representation power while maintaining injectivity. We will therefore look at a much more drastic\n",
    "and interesting approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to understand how we could create a function that is injective. The first step is to\n",
    "realize that:\n",
    "> A continuous function is injective if and only if it is strictly monotonic.[^injective_monotonic]\n",
    "\n",
    "Therefore we can create injective functions by creating strictly monotonic functions.\n",
    "\n",
    "[^injective_monotonic]: Math StackExchange,\n",
    "    [Continuous injective map is strictly monotonic](https://math.stackexchange.com/questions/752073/continuous-injective-map-is-strictly-monotonic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monotonic Networks: A Superpower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create injective activation functions using a drastic measure. Every activation function\n",
    "will be an injective neural network!\n",
    "\n",
    "We will create neural networks that take in a scalar input and output a scalar value and are\n",
    "strictly monotonic in the input. Mathematically our activation function now is a neural network $M$\n",
    "$$\n",
    "M: \\mathbb{R} \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "which satisfies one of:\n",
    "$$\n",
    "M(x_1) < M(x_2) \\quad \\forall x_1, x_2 \\in \\mathbb{R} \\quad s.t. \\; x_1 < x_2\\\\\n",
    "M(x_1) > M(x_2) \\quad \\forall x_1, x_2 \\in \\mathbb{R} \\quad s.t. \\; x_1 > x_2\n",
    "$$\n",
    "\n",
    ":::{card}\n",
    ":header: Monotonic Networks\n",
    ":footer: [@sill1997monotonic; @igel2023smooth; @runje2023constrained]\n",
    "We will choose neural networks that _by design_ for _any_ parameter value are always monotonic,\n",
    "these are called **_Monotonic Networks_**.\n",
    ":::\n",
    "\n",
    "Every activation layer will be an independent Monotonic Network and all neurons will use the same\n",
    "neural network. This is shown in [](#neural_injective_geometry_net).\n",
    ":::{figure} assets/neural_injective_geometry_net\n",
    ":label: neural_injective_geometry_net\n",
    ":alt: Injective Networks with Monotonic Networks as activation functions\n",
    ":height: 210px\n",
    "Injective Network with Monotonic Networks as activation functions. $M_1$ shown in the figure is a\n",
    "monotonic network.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Monotonic Networks with Smooth Min-Max Networks\n",
    "Building Monotonic Networks is an active area of research and there are a few popular choices. The\n",
    "first Monotonic Network was designed by @sill1997monotonic which used $min$ and $max$ operations to\n",
    "impart monotonicity. But since the $min-max$ operations are not differentiable they are hard\n",
    "to train and also suffer from dead neurons[^dead_neurons]. Recently @igel2023smooth proposed a\n",
    "variation of the original Monotonic Networks proposed by @sill1997monotonic, which replaces the hard\n",
    "$min-max$ with their smooth variants. These are called Smooth Min-Max Monotonic Networks.\n",
    "\n",
    "We will use Smooth Min-Max Monotonic Networks as activation functions to augment Injective Networks.\n",
    "We use the code provided in Christian's Repository\n",
    "[https://github.com/christian-igel/SMM](https://github.com/christian-igel/SMM) as reference for our\n",
    "own code.\n",
    "\n",
    "[^dead_neurons]: Data Science StackExchange\n",
    "    [What is the \"dying ReLU\" problem in neural networks?](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
